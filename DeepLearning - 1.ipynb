{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large already present -  Skipping extraction of notMNIST_large.tar.gz.\n",
      "['notMNIST_large\\\\A', 'notMNIST_large\\\\B', 'notMNIST_large\\\\C', 'notMNIST_large\\\\D', 'notMNIST_large\\\\E', 'notMNIST_large\\\\F', 'notMNIST_large\\\\G', 'notMNIST_large\\\\H', 'notMNIST_large\\\\I', 'notMNIST_large\\\\J']\n",
      "notMNIST_small already present -  Skipping extraction of notMNIST_small.tar.gz.\n",
      "['notMNIST_small\\\\A', 'notMNIST_small\\\\B', 'notMNIST_small\\\\C', 'notMNIST_small\\\\D', 'notMNIST_small\\\\E', 'notMNIST_small\\\\F', 'notMNIST_small\\\\G', 'notMNIST_small\\\\H', 'notMNIST_small\\\\I', 'notMNIST_small\\\\J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force = False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present -  Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception(\n",
    "        'Expected %d folders, one per class. Found %d instead.' % (\n",
    "            num_classes, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "train_folder = maybe_extract('notMNIST_large.tar.gz')\n",
    "test_folder = maybe_extract('notMNIST_small.tar.gz')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified .\\notMNIST_large.tar.gz\n",
      "Found and verified .\\notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.'\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "    \n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 ==0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        last_percent_reported = percent\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force = False):\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print ('Attempting to download:', filename)\n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception('Failed to verify '+ dest_filename + '. can you get to it with a browser?')\n",
    "    return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz',247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz',8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABKElEQVR4nHXRPU4DMRAF4DceBykS\nHISKn1U4Aw11FBoaroFIhSjpaaiyoqOnoSEH4BK0ILQSwet5FLZ3idh9leVPY3vGuIETjIV3kFFt\neS/ixioDa8WIkoFP06yiqqrqeuQPn/cgALq780IIoPWvZx+gM3cEAAhvAuZKsuUMKqIrpiyh0qHF\nCl60ZohmFlteJ03ISlRqhlRogUt46XAGvyqWVaXgAR65sRJayyV8RqseuBULvEqtAHF90mg3GN0F\naHqbkHK6nsRC8fCFAlB83nn/+jPSzzQfK7gj2lfmTVeQJPtlXhV0/Ve47gMLNrGciogmndA96Hjq\nrFTafrLS5/8Q8RJlIlsho8U5wMFEswUmw2gWz+GHK422wASDaLaZpx6HLvy+SPYLx4dMEQPVVH8A\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_index= 1\n",
    "image_index = 0\n",
    "\n",
    "image_sample = os.listdir(train_folder[folder_index])[image_index]\n",
    "image_path = os.path.join(train_folder[folder_index], image_sample)\n",
    "Image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large\\A.pickle already present - skipping pickling\n",
      "notMNIST_large\\B.pickle already present - skipping pickling\n",
      "notMNIST_large\\C.pickle already present - skipping pickling\n",
      "notMNIST_large\\D.pickle already present - skipping pickling\n",
      "notMNIST_large\\E.pickle already present - skipping pickling\n",
      "notMNIST_large\\F.pickle already present - skipping pickling\n",
      "notMNIST_large\\G.pickle already present - skipping pickling\n",
      "notMNIST_large\\H.pickle already present - skipping pickling\n",
      "notMNIST_large\\I.pickle already present - skipping pickling\n",
      "notMNIST_large\\J.pickle already present - skipping pickling\n",
      "notMNIST_small\\A.pickle already present - skipping pickling\n",
      "notMNIST_small\\B.pickle already present - skipping pickling\n",
      "notMNIST_small\\C.pickle already present - skipping pickling\n",
      "notMNIST_small\\D.pickle already present - skipping pickling\n",
      "notMNIST_small\\E.pickle already present - skipping pickling\n",
      "notMNIST_small\\F.pickle already present - skipping pickling\n",
      "notMNIST_small\\G.pickle already present - skipping pickling\n",
      "notMNIST_small\\H.pickle already present - skipping pickling\n",
      "notMNIST_small\\I.pickle already present - skipping pickling\n",
      "notMNIST_small\\J.pickle already present - skipping pickling\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "pixel_depth = 255.0\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape = (len(image_files), image_size, image_size), dtype = np.float32)\n",
    "    \n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - pixel_depth/2)/pixel_depth\n",
    "            if image_data.shape !=(image_size, image_size):\n",
    "                raise Exception('Unexpected image shape %s' % str(image_data.shape))\n",
    "            dataset[num_images,:,:] = image_data\n",
    "            num_images = num_images+1\n",
    "        except IOError as e:\n",
    "            print('Could not read ', image_file, ':', e, 'It\\'s ok. Skipping')\n",
    "    \n",
    "    dataset = dataset[0:num_images,:,:]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Fewer images than required %d < %d' % (num_images, min_num_images))\n",
    "        \n",
    "    print('Full dataset sensor', dataset.shape)\n",
    "    print('Mean: ', np.mean(dataset))\n",
    "    print('Std: ',np.std(dataset) )\n",
    "    return dataset\n",
    "\n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force = False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder +'.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            print('%s already present - skipping pickling' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s ' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename,'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to file ', set_filename, ':', e)\n",
    "    return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folder, 45000)\n",
    "test_datasets = maybe_pickle(test_folder, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d4f2bef278>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0ERfiAowg8E9f+iGAcUxA+p6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d4f2b30908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = train_folder[folder_index]\n",
    "image = os.listdir(filename)[image_index]\n",
    "image_file  =os.path.join(filename, image)\n",
    "\n",
    "image_data = (ndimage.imread(image_file))\n",
    "plt.imshow(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52912, 52912, 52912, 52912, 52912, 52912, 52912, 52912, 52912, 52911]\n"
     ]
    }
   ],
   "source": [
    "size_class = []\n",
    "for folder in train_folder:\n",
    "    images = os.listdir(folder)\n",
    "    size_class.append(len(images))\n",
    "\n",
    "print(size_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  (200000, 28, 28) (200000,)\n",
      "Validating:  (10000, 28, 28) (10000,)\n",
      "Testing:  (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype = np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype = np.float32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size = 0):\n",
    "    num_classes  = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    \n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    \n",
    "    start_v, start_t = 0,0 \n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class + tsize_per_class\n",
    "    \n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file,'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class,:,:]\n",
    "                    valid_dataset[start_v:end_v,:,:] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v +=vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "            \n",
    "                train_letter = letter_set[vsize_per_class:end_l,:,:]\n",
    "                train_dataset[start_t:end_t,:,:] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t +=tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from ', pickle_file,':', e)\n",
    "            raise\n",
    "    \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "\n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training: ', train_dataset.shape, train_labels.shape)\n",
    "print('Validating: ', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing: ', test_dataset.shape, test_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset_shuffle, train_labels_shuffle = randomize(train_dataset, train_labels)\n",
    "valid_dataset_shuffle, valid_labels_shuffle = randomize(valid_dataset, valid_labels)\n",
    "test_dataset_shuffle, test_labels_shuffle = randomize(test_dataset, test_labels)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def image_dist(image1, image2):\n",
    "    return np.sum((image1 - image2)**2)\n",
    "\n",
    "image_one = train_dataset_shuffle[0,:,:]\n",
    "label_one = train_labels_shuffle[0]\n",
    "\n",
    "for index in range(0,train_dataset.shape[0]):\n",
    "    if image_dist(image_one, train_dataset[index,:,:])==0:\n",
    "        if label_one == train_labels[index]:\n",
    "            print('Success')\n",
    "        else:\n",
    "            print('Shuffle failed')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
